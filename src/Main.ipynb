{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from ml.dataset.mnist import load as load_mnist, init as init_mnist\n",
    "import seaborn as sbs\n",
    "import numpy as np\n",
    "\n",
    "MNIST_PATH = \"../data/mnist\"\n",
    "x_train, y_train, x_test, y_test = load_mnist(MNIST_PATH)\n",
    "x_train = np.array(((x_train / 256) - 0.5) / 0.5)  # Normalize\n",
    "y_train = np.array([[int(x == y) for x in range(10)] for y in y_train])\n",
    "x_test = np.array(((x_test / 256) - 0.5) / 0.5)  # Normalize\n",
    "y_test = np.array([[int(x == y) for x in range(10)] for y in y_test])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from ml.nn.base import NeuralNetwork\n",
    "from ml.nn.optim import SGD, Adagrad\n",
    "from ml.nn.layer.linear import Linear\n",
    "from ml.nn.loss import MSELoss, BinaryCrossEntropyLoss\n",
    "from ml.nn.activation import Softmax, ReLU, LeakyReLU, Sigmoid\n",
    "import random\n",
    "import json\n",
    "\n",
    "nn = NeuralNetwork([\n",
    "    Linear(784, 512),\n",
    "    LeakyReLU(),\n",
    "    Linear(512, 10),\n",
    "    Softmax()\n",
    "], BinaryCrossEntropyLoss(), SGD(1e-4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1; 10240/60000; 0.05559                               "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 41\u001B[0m\n\u001B[0;32m     36\u001B[0m             f\u001B[38;5;241m.\u001B[39mwrite(json\u001B[38;5;241m.\u001B[39mdumps(nn\u001B[38;5;241m.\u001B[39mstate_dict()))\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_data, loss_epoch\n\u001B[1;32m---> 41\u001B[0m loss_on_iter, loss_on_epoch \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_nn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 24\u001B[0m, in \u001B[0;36mtrain_nn\u001B[1;34m(batch_size, epoch_size, save_path)\u001B[0m\n\u001B[0;32m     22\u001B[0m nn\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     23\u001B[0m loss_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m; \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m; \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\u001B[38;5;250m \u001B[39m\u001B[38;5;241m5\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m30\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     26\u001B[0m loss_data[loss_data_on] \u001B[38;5;241m=\u001B[39m loss\n\u001B[0;32m     27\u001B[0m loss_data_on \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def train_nn(batch_size: int, epoch_size: int, save_path: str = \"../model.json\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data_size = len(x_train)\n",
    "    data_pos = np.arange(len(x_train))\n",
    "\n",
    "    loss_data = np.zeros(len(range(0, data_size, batch_size)) * epoch_size)\n",
    "    loss_data_on = 0\n",
    "\n",
    "    loss_epoch = np.zeros(epoch_size)\n",
    "\n",
    "    for epoch in range(1, epoch_size + 1):\n",
    "        np.random.shuffle(data_pos)\n",
    "        loss_total = 0.\n",
    "        for i in range(0, data_size, batch_size):\n",
    "            p = data_pos[i:min(i + batch_size, data_size)]\n",
    "            nn.zero_grad()\n",
    "            result = nn(x_train[p])\n",
    "            loss = nn.loss(y_train[p])\n",
    "            nn.backward()\n",
    "            nn.step()\n",
    "            loss_total += loss\n",
    "            print(f\"\\r{epoch}; {i}/{data_size}; {round(loss.item(), 5)} {' ' * 30}\", end=\"\")\n",
    "\n",
    "            loss_data[loss_data_on] = loss\n",
    "            loss_data_on += 1\n",
    "\n",
    "        loss_total /= len(range(0, data_size, batch_size))\n",
    "        print(f\"\\r{epoch}; Loss total: {round(loss_total, 5)}. {' ' * 30}\")\n",
    "\n",
    "        loss_epoch[epoch - 1] = loss_total\n",
    "\n",
    "    if save_path is not None:\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(json.dumps(nn.state_dict()))\n",
    "\n",
    "    return loss_data, loss_epoch\n",
    "\n",
    "\n",
    "loss_on_iter, loss_on_epoch = train_nn(1024, 20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sbs.lineplot(y=loss_on_iter, x=np.arange(loss_on_iter.shape[0]) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sbs.lineplot(y=loss_on_epoch, x=np.arange(len(loss_on_epoch)) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def test_nn(batch_size: int = 256, model_path: str = \"../model.json\",\n",
    "            data_set: Tuple[np.ndarray, np.ndarray] = (x_test, y_test)) -> Tuple[int, int, int, float]:\n",
    "    with open(model_path, \"r+\") as f:\n",
    "        nn.load_state_dict(json.load(f))\n",
    "    data, lbl = data_set\n",
    "\n",
    "    total = len(data)\n",
    "    data_pos = np.arange(total)\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0.\n",
    "    epoch_cnt = len(range(0, total, batch_size))\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(0, total, batch_size):\n",
    "        p = data_pos[i: min(i + batch_size, total)]\n",
    "        x = data[p]\n",
    "        y = lbl[p]\n",
    "\n",
    "        nn.zero_grad()\n",
    "        result = nn(x)\n",
    "\n",
    "        loss_round = nn.loss(y)\n",
    "\n",
    "        loss += loss_round\n",
    "        correct_round = np.sum(np.argmax(result, axis=1) == np.argmax(y, axis=1))\n",
    "        correct += correct_round\n",
    "\n",
    "        cnt += 1\n",
    "        print(f\"\\r{cnt}/{epoch_cnt}; Loss: {round(loss_round, 5)}; \"\n",
    "              f\"Accuracy: {round(correct_round / x.shape[0], 5)} {' ' * 30}\", end=\"\")\n",
    "\n",
    "    print()\n",
    "    print(f\"Test Result:\")\n",
    "    print(f\"Correct: {correct}\")\n",
    "    print(f\"Wrong: {total - correct}\")\n",
    "    print(f\"Loss: {round(loss / epoch_cnt, 5)}\")\n",
    "    print(f\"Accuracy: {round(correct / total, 5)}\")\n",
    "\n",
    "    return total, correct, total - correct, loss\n",
    "\n",
    "\n",
    "test_nn()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare to traditional algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def report(model):\n",
    "    out, targ = np.argmax(y_test, axis=1), np.argmax(lin_reg.predict(x_test), axis=1)\n",
    "    report = classification_report(out, targ)\n",
    "    print(report)\n",
    "    correct = np.sum(out == targ)\n",
    "    return out.shape[0], correct, out.shape[0] - correct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total, lin_reg_correct, lin_reg_wrong = report(lin_reg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total, forest_correct, forest_wrong = report(forest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_tree = DecisionTreeClassifier()\n",
    "d_tree.fit(x_train, np.argmax(y_train, axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_tree_result = d_tree.predict(x_test)\n",
    "d_tree_correct = np.sum(d_tree_result == np.argmax(y_test, axis=1))\n",
    "d_tree_wrong = total - d_tree_correct\n",
    "print(classification_report(np.argmax(y_test, axis=1), d_tree_result))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
