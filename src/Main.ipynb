{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\basic\\Lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ml.dataset.mnist import load as load_mnist, init as init_mnist\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "\n",
    "MNIST_PATH = \"../data/mnist\"\n",
    "x_train, y_train, x_test, y_test = load_mnist(MNIST_PATH)\n",
    "x_train = np.array(((x_train / 256) - 0.5) / 0.5)  # Normalize\n",
    "y_train = np.array([[int(x == y) for x in range(10)] for y in y_train])\n",
    "x_test = np.array(((x_test / 256) - 0.5) / 0.5)  # Normalize\n",
    "y_test = np.array([[int(x == y) for x in range(10)] for y in y_test])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ml.nn.base import NeuralNetwork\n",
    "from ml.nn.optim import SGD, Adagrad\n",
    "from ml.nn.layer.linear import Linear\n",
    "from ml.nn.loss import MSELoss, BinaryCrossEntropyLoss\n",
    "from ml.nn.activation import Softmax, ReLU, LeakyReLU, Sigmoid\n",
    "import random\n",
    "import json\n",
    "\n",
    "nn = NeuralNetwork([\n",
    "    Linear(784, 512),\n",
    "    LeakyReLU(),\n",
    "    Linear(512, 10),\n",
    "    Softmax()\n",
    "], BinaryCrossEntropyLoss(), SGD(1e-4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "256\n",
      "512\n",
      "768\n",
      "1024\n",
      "1280\n",
      "1536\n",
      "1792\n",
      "2048\n",
      "2304\n",
      "2560\n",
      "2816\n",
      "3072\n",
      "3328\n",
      "3584\n",
      "3840\n",
      "4096\n",
      "4352\n",
      "4608\n",
      "4864\n",
      "5120\n",
      "5376\n",
      "5632\n",
      "5888\n",
      "6144\n",
      "6400\n",
      "6656\n",
      "6912\n",
      "7168\n",
      "7424\n",
      "7680\n",
      "7936\n",
      "8192\n",
      "8448\n",
      "8704\n",
      "8960\n",
      "9216\n",
      "9472\n",
      "9728\n",
      "9984\n",
      "10240\n",
      "10496\n",
      "10752\n",
      "11008\n",
      "11264\n",
      "11520\n",
      "11776\n",
      "12032\n",
      "12288\n",
      "12544\n",
      "12800\n",
      "13056\n",
      "13312\n",
      "13568\n",
      "13824\n",
      "14080\n",
      "14336\n",
      "14592\n",
      "14848\n",
      "15104\n",
      "15360\n",
      "15616\n",
      "15872\n",
      "16128\n",
      "16384\n",
      "16640\n",
      "16896\n",
      "17152\n",
      "17408\n",
      "17664\n",
      "17920\n",
      "18176\n",
      "18432\n",
      "18688\n",
      "18944\n",
      "19200\n",
      "19456\n",
      "19712\n",
      "19968\n",
      "20224\n",
      "20480\n",
      "20736\n",
      "20992\n",
      "21248\n",
      "21504\n",
      "21760\n",
      "22016\n",
      "22272\n",
      "22528\n",
      "22784\n",
      "23040\n",
      "23296\n",
      "23552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def train_nn(batch_size: int, epoch_size: int, save_path: str = \"../model.json\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data_size = len(x_train)\n",
    "    data_pos = np.arange(len(x_train))\n",
    "\n",
    "    loss_data = np.zeros(len(range(0, data_size, batch_size)) * epoch_size)\n",
    "    loss_data_on = 0\n",
    "\n",
    "    loss_epoch = np.zeros(epoch_size)\n",
    "\n",
    "    for epoch in range(1, epoch_size + 1):\n",
    "        np.random.shuffle(data_pos)\n",
    "        loss_total = 0.\n",
    "        for i in range(0, data_size, batch_size):\n",
    "            p = data_pos[i:min(i + batch_size, data_size)]\n",
    "            nn.zero_grad()\n",
    "            result = nn(x_train[p])\n",
    "            loss = nn.loss(y_train[p])\n",
    "            nn.backward()\n",
    "            nn.step()\n",
    "            loss_total += loss\n",
    "            # print(f\"\\r{epoch}; {i}/{data_size}; {round(loss.item(), 5)} {' ' * 30}\", end=\"\")\n",
    "\n",
    "            loss_data[loss_data_on] = loss\n",
    "            loss_data_on += 1\n",
    "\n",
    "        loss_total /= len(range(0, data_size, batch_size))\n",
    "        print(f\"\\r{epoch}; Loss total: {round(loss_total, 5)}. {' ' * 30}\")\n",
    "\n",
    "        loss_epoch[epoch - 1] = loss_total\n",
    "\n",
    "    if save_path is not None:\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(json.dumps(nn.state_dict()))\n",
    "\n",
    "    return loss_data, loss_epoch\n",
    "\n",
    "\n",
    "loss_on_iter, loss_on_epoch = train_nn(256, 20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sbs.lineplot(y=loss_on_iter, x=np.arange(loss_on_iter.shape[0]) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sbs.lineplot(y=loss_on_epoch, x=np.arange(len(loss_on_epoch)) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def test_nn(batch_size: int = 256, model_path: str = \"../model.json\",\n",
    "            data_set: Tuple[np.ndarray, np.ndarray] = (x_test, y_test)) -> Tuple[int, int, int, float]:\n",
    "    with open(model_path, \"r+\") as f:\n",
    "        nn.load_state_dict(json.load(f))\n",
    "    data, lbl = data_set\n",
    "\n",
    "    total = len(data)\n",
    "    data_pos = np.arange(total)\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0.\n",
    "    epoch_cnt = len(range(0, total, batch_size))\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(0, total, batch_size):\n",
    "        p = data_pos[i: min(i + batch_size, total)]\n",
    "        x = data[p]\n",
    "        y = lbl[p]\n",
    "\n",
    "        nn.zero_grad()\n",
    "        result = nn(x)\n",
    "\n",
    "        loss_round = nn.loss(y)\n",
    "\n",
    "        loss += loss_round\n",
    "        correct_round = np.sum(np.argmax(result, axis=1) == np.argmax(y, axis=1))\n",
    "        correct += correct_round\n",
    "\n",
    "        cnt += 1\n",
    "        print(f\"\\r{cnt}/{epoch_cnt}; Loss: {round(loss_round, 5)}; \"\n",
    "              f\"Accuracy: {round(correct_round / x.shape[0], 5)} {' ' * 30}\", end=\"\")\n",
    "\n",
    "    print()\n",
    "    print(f\"Test Result:\")\n",
    "    print(f\"Correct: {correct}\")\n",
    "    print(f\"Wrong: {total - correct}\")\n",
    "    print(f\"Loss: {round(loss / epoch_cnt, 5)}\")\n",
    "    print(f\"Accuracy: {round(correct / total, 5)}\")\n",
    "\n",
    "    return total, correct, total - correct, loss\n",
    "\n",
    "\n",
    "test_nn()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare to traditional algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def report(model):\n",
    "    out, targ = np.argmax(y_test, axis=1), np.argmax(lin_reg.predict(x_test), axis=1)\n",
    "    report = classification_report(out, targ)\n",
    "    print(report)\n",
    "    correct = np.sum(out == targ)\n",
    "    return out.shape[0], correct, out.shape[0] - correct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total, lin_reg_correct, lin_reg_wrong = report(lin_reg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total, forest_correct, forest_wrong = report(forest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_tree = DecisionTreeClassifier()\n",
    "d_tree.fit(x_train, np.argmax(y_train, axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_tree_result = d_tree.predict(x_test)\n",
    "d_tree_correct = np.sum(d_tree_result == np.argmax(y_test, axis=1))\n",
    "d_tree_wrong = total - d_tree_correct\n",
    "print(classification_report(np.argmax(y_test, axis=1), d_tree_result))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
