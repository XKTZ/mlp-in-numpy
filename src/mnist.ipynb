{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from mlnp.device import set_default_continuous_type\n",
    "import numpy as np\n",
    "\n",
    "set_default_continuous_type(np.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from mlnp.dataset.mnist import load as load_mnist, init as init_mnist, download_mnist\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "MNIST_PATH = \"../data/mnist\"\n",
    "\n",
    "if not os.path.exists(MNIST_PATH):\n",
    "    os.mkdir(MNIST_PATH)\n",
    "    init_mnist(MNIST_PATH)\n",
    "\n",
    "x_train, y_train_idx, x_test, y_test_idx = load_mnist(MNIST_PATH)\n",
    "# x_train = np.pad(x_train, ((0, 0), (0, 0), (2, 2), (2, 2)))\n",
    "x_train = ((x_train / 256) - 0.5) / 0.5  # Normalize\n",
    "y_train = np.zeros((x_train.shape[0], 10), dtype=np.float32)\n",
    "y_train[np.arange(x_train.shape[0]), y_train_idx] = 1.\n",
    "# x_test = np.pad(x_test, ((0, 0), (0, 0), (2, 2), (2, 2)))\n",
    "x_test = ((x_test / 256) - 0.5) / 0.5  # Normalize\n",
    "y_test = np.zeros((x_test.shape[0], 10), dtype=np.float32)\n",
    "y_test[np.arange(x_test.shape[0]), y_test_idx] = 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from mlnp.nn.base import NeuralNetwork\n",
    "from mlnp.nn.loss import BinaryCrossEntropyLoss, MSELoss\n",
    "from mlnp.nn.layer import Softmax, LeakyReLU, Dropout, Sigmoid, Linear, Conv2d, Reshape, Flatten, MaxPool2d\n",
    "from mlnp.nn.optim import Adam, SGD\n",
    "import json\n",
    "\n",
    "nn = NeuralNetwork([\n",
    "    Flatten(),\n",
    "    Linear(28 * 28, 512),\n",
    "    LeakyReLU(0.2),\n",
    "    Linear(512, 10),\n",
    "    LeakyReLU(0.2),\n",
    "    Softmax()\n",
    "], BinaryCrossEntropyLoss(), SGD(1e-4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('float32')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn._layers[1].mat.dtype"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1; 3364/60000; 0.10029                                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programming\\numpy\\MLP\\src\\ml\\nn\\optim.py:263: RuntimeWarning: divide by zero encountered in divide\n",
      "  gs = tuple(np.where(np.abs(g) < 1e-3, 0, 1. / g) for g in gs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1; 14568/60000; 0.02089                                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import time\n",
    "\n",
    "def train_nn(batch_size: int, epoch_size: int, save_path: str = \"../model.pkl\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data_size = len(x_train)\n",
    "    data_pos = np.arange(len(x_train))\n",
    "\n",
    "    loss_data = np.zeros(len(range(0, data_size, batch_size)) * epoch_size)\n",
    "    loss_data_on = 0\n",
    "\n",
    "    loss_epoch = np.zeros(epoch_size)\n",
    "\n",
    "    nn.train()\n",
    "\n",
    "    for epoch in range(1, epoch_size + 1):\n",
    "        np.random.shuffle(data_pos)\n",
    "        loss_total = 0.\n",
    "        for i in range(0, data_size, batch_size):\n",
    "            p = data_pos[i:min(i + batch_size, data_size)]\n",
    "            nn.zero_grad()\n",
    "            result = nn(x_train[p])\n",
    "            loss = nn.loss(y_train[p])\n",
    "            nn.backward()\n",
    "            nn.step()\n",
    "            loss_total += loss.item()\n",
    "            print(f\"\\r{epoch}; {i}/{data_size}; {round(loss.item(), 5)} {' ' * 30}\", end=\"  \")\n",
    "\n",
    "            loss_data[loss_data_on] = loss\n",
    "            loss_data_on += 1\n",
    "\n",
    "        loss_total /= len(range(0, data_size, batch_size))\n",
    "        print(f\"\\r{epoch}; Loss total: {round(loss_total, 5)}. {' ' * 30}\")\n",
    "\n",
    "        loss_epoch[epoch - 1] = loss_total\n",
    "\n",
    "    if save_path is not None:\n",
    "        with open(save_path, \"wb+\") as f:\n",
    "            pickle.dump(nn.state_dict(), f)\n",
    "\n",
    "    return loss_data, loss_epoch\n",
    "\n",
    "\n",
    "loss_on_iter, loss_on_epoch = train_nn(4, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sbs.lineplot(y=loss_on_iter, x=np.arange(loss_on_iter.shape[0]) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sbs.lineplot(y=loss_on_epoch, x=np.arange(len(loss_on_epoch)) + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40; Loss: 0.010649999603629112; Accuracy: 0.875                                  \n",
      "Test Result:\n",
      "Correct: 8292\n",
      "Wrong: 1708\n",
      "Loss: 0.02304\n",
      "Accuracy: 0.8292\n"
     ]
    },
    {
     "data": {
      "text/plain": "(10000, 8292, 1708, 0.9216675735078752)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def test_nn(batch_size: int = 256, model_path: str = \"../model.pkl\",\n",
    "            data_set: Tuple[np.ndarray, np.ndarray] = (x_test, y_test)) -> Tuple[int, int, int, float]:\n",
    "    # with open(model_path, \"rb+\") as f:\n",
    "    #     nn.load_state_dict(pickle.load(f))\n",
    "\n",
    "    data, lbl = data_set\n",
    "\n",
    "    total = len(data)\n",
    "    data_pos = np.arange(total)\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0.\n",
    "    epoch_cnt = len(range(0, total, batch_size))\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    nn.eval()\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        p = data_pos[i: min(i + batch_size, total)]\n",
    "        x = data[p]\n",
    "        y = lbl[p]\n",
    "\n",
    "        nn.zero_grad()\n",
    "        result = nn(x)\n",
    "\n",
    "        loss_round = nn.loss(y)\n",
    "\n",
    "        loss += loss_round\n",
    "        correct_round = np.sum(np.argmax(result, axis=1) == np.argmax(y, axis=1))\n",
    "        correct += correct_round\n",
    "\n",
    "        cnt += 1\n",
    "        print(f\"\\r{cnt}/{epoch_cnt}; Loss: {round(loss_round, 5)}; \"\n",
    "              f\"Accuracy: {round(correct_round / x.shape[0], 5)} {' ' * 30}\", end=\"\")\n",
    "\n",
    "    print()\n",
    "    print(f\"Test Result:\")\n",
    "    print(f\"Correct: {correct}\")\n",
    "    print(f\"Wrong: {total - correct}\")\n",
    "    print(f\"Loss: {round(loss / epoch_cnt, 5)}\")\n",
    "    print(f\"Accuracy: {round(correct / total, 5)}\")\n",
    "\n",
    "    return total, correct, total - correct, loss\n",
    "\n",
    "\n",
    "test_nn()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare to traditional algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def report(model):\n",
    "    out, targ = np.argmax(y_test, axis=1), np.argmax(lin_reg.predict(x_test), axis=1)\n",
    "    report = classification_report(out, targ)\n",
    "    print(report)\n",
    "    correct = np.sum(out == targ)\n",
    "    return out.shape[0], correct, out.shape[0] - correct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total, lin_reg_correct, lin_reg_wrong = report(lin_reg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total, forest_correct, forest_wrong = report(forest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_tree = DecisionTreeClassifier()\n",
    "d_tree.fit(x_train, np.argmax(y_train, axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_tree_result = d_tree.predict(x_test)\n",
    "d_tree_correct = np.sum(d_tree_result == np.argmax(y_test, axis=1))\n",
    "d_tree_wrong = total - d_tree_correct\n",
    "print(classification_report(np.argmax(y_test, axis=1), d_tree_result))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
